{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7615663,"sourceType":"datasetVersion","datasetId":4435113}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport string\n\nimport langid\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom collections import Counter\nfrom scipy import stats\nfrom dateutil import parser\n\nimport seaborn as sns\n\nimport shapely\nfrom shapely.geometry import Point\nimport geopandas as gpd","metadata":{"execution":{"iopub.status.busy":"2024-02-13T09:38:17.314049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read In Data\n* **TokyoAirbnbListings2023.csv**: includes description of listings, transit, if superhost...\n* **TokyoAirbnbReviews2023.csv**: includes listing id and unique id for each reviewer with comments and dates","metadata":{}},{"cell_type":"code","source":"listings = pd.read_csv('/kaggle/input/tokyo-airbnb-neighborhoods/TokyoAirbnbListings2023.csv')\nreviews = pd.read_csv(\"/kaggle/input/tokyo-airbnb-neighborhoods/TokyoAirbnbReviews2023.csv\")\nreviews = reviews.tail(5000)\n\n# Reset the index so it starts from 0\nreviews.reset_index(drop=True, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listings.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews['comments'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Reviews for Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"reviews.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews['comments'].str.match('The host canceled this reservation').sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.dropna(subset =['comments'], how='any', axis=0, inplace=True)\n\nindex_canceled = reviews[ reviews['comments'].str.match('The host canceled this reservation')].index\nreviews.drop(index_canceled, inplace=True)\n\nindex_dash = reviews[ reviews['comments'].str.match('-')].index\nreviews.drop(index_dash, inplace=True)\n\nalphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\nreviews['comments'] = reviews['comments'].map(alphanumeric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_errors = 0\nerr = []\n\nfor index, row in reviews.iterrows():\n    try:\n        # Use langid to classify the language of the comment\n        language, _ = langid.classify(row['comments'])\n    except Exception as e:\n        num_errors += 1\n        err.append(index)\n        print(f\"This row throws an error: {row['comments']}\")\n        print(f\"Error message: {str(e)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('{:.2f}% of entries for language identification throw errors'.format(num_errors/reviews['comments'].shape[0] * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.drop(err, axis=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_language(comment):\n    try:\n        # Return only the language code\n        language, _ = langid.classify(comment)\n        return language\n    except Exception as e:\n        # Handle the exception if any and return a NaN or some default value\n        print(f\"Error processing comment: {comment}, Error: {e}\")\n        return None  # or return 'unknown' or similar\n\n# Apply the function to the 'comments' column and create a new 'language' column\nreviews['language'] = reviews['comments'].apply(detect_language)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews['language'].value_counts().plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a copy in case we eventually want to try translating the non-English rows instead of dropping them\n#reviews_en = reviews.copy(deep=True)\n\n# isolate all non-en entries\nindex_nonen = reviews[~reviews['language'].str.match('en')].index\n\nprint('{:.2f}% of all entries are not in English'.format(len(index_nonen)/reviews.shape[0]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop\nreviews.drop(index_nonen, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Estimating Polarity\n\nhttps://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n\n**VADER** (Valence Aware Dictionary and sEntiment Reasoner) is a sentiment analysis tool designed with a focus on social media content, employing a lexicon and rule-based approach. It utilizes a sentiment lexicon, which is essentially a collection of words each tagged with their semantic orientation as positive or negative.\n\nThis tool has proven to be highly effective for analyzing texts from social media, as well as content from NY Times editorials, movie reviews, and product reviews. VADER excels by not just providing scores for positivity and negativity, but also by quantifying the degree of sentiment expressed.\n\nVADER enhances analysis through several unique features: it adjusts scores for words based on capitalization, punctuation (for example, increasing the compound score with the addition of exclamation points), the use of degree modifiers (comparing phrases like \"very good\" versus \"good\"), and the presence of emojis. Additionally, it effectively manages shifts in sentiment polarity when conjunctions are used, such as in \"but\" phrases, to indicate a change in sentiment direction.","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test it out on first comment\n\ndef sentiment_analyzer_scores(comment):\n    score = analyzer.polarity_scores(comment)\n    return(pd.DataFrame.from_dict(score, orient='index'))\n\nsentiment_analyzer_scores(reviews['comments'][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we want one overall polarity score, so just look at the 'compound' score\n\npolarity_compound = lambda s: (analyzer.polarity_scores(s))['compound']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews['polarity'] = reviews.comments.map(polarity_compound)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews[:5].style.bar(subset=['polarity'], align='mid', color=['#d65f5f', '#5fba7d'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews[reviews['polarity']<0][:5].style.bar(subset=['polarity'], align='mid', color=['#d65f5f', '#5fba7d'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(reviews['polarity'], norm_hist=True, color='g')\nplt.title('Distribution of sentiment polarity');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews.to_csv('reviews_polarity.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Investigate the polarity variable and its relationship with other attributes, such as various scores. \n\nExtract from TokyoAirbnbListings2023.csv all columns that represent specific aspects (accuracy, cleanliness, check-in, communication, location, and value) as well as the aggregate (rating) scores. Ensure to include the 'id' column for integration with additional DataFrames and the 'neighbourhood' column for subsequent analysis.","metadata":{}},{"cell_type":"code","source":"sel_listings = listings[['id', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', \n                         'review_scores_cleanliness', 'review_scores_checkin', \n                         'review_scores_communication', 'review_scores_location', 'review_scores_value',\n                         'neighbourhood_cleansed']]\nsel_reviews = reviews[['listing_id', 'reviewer_id', 'comments', 'polarity']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel_reviews.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel_listings.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'polarity' to numeric\nsel_reviews.loc[:, 'polarity'] = pd.to_numeric(sel_reviews['polarity'], errors='coerce')\n\n# Group by 'listing_id' to calculate the mean polarity\nlist_pol = sel_reviews.groupby('listing_id', as_index=False)['polarity'].mean()\n\n# Merge list_pol with sel_listings on 'id' and 'listing_id'\nfull = pd.merge(sel_listings, list_pol, left_on='id', right_on='listing_id', how='left')\n\n# Check the first few rows to confirm the merge is correct\nfull.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_scores = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n                 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', \n                 'review_scores_value', 'polarity']\nprint(review_scores)\n\ncorr = full[review_scores].corr()\n\nsns.heatmap(corr, annot=True)\nplt.title('Pearson correlation between score features and polarity');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full['review_scores_rating'].hist();\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full['polarity'].hist();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up dataframe for dashboard\n","metadata":{}},{"cell_type":"code","source":"# drop rows with null polarity\nfull.dropna(subset = ['polarity'], how='any', axis=0, inplace=True)\n\nfull","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='polarity', y='number_of_reviews', data=full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(listings.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selected columns from listings\ndashboard_df = pd.DataFrame(listings[['id', 'price','minimum_nights', 'maximum_nights', 'host_is_superhost']])\n\n# remove $ sign\n# and fill null values in security_deposit and cleaning_fee with 0\ncols = ['price']\nfor col in cols:\n    dashboard_df[col] = dashboard_df[col].str.replace('$', '')\n    dashboard_df[col].fillna(0, inplace=True)\n\n# transform host_is_superhost to boolean \ndashboard_df.replace({'host_is_superhost': {'f': False, 't': True}}, inplace=True)\n\ndashboard_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dashboard_df = pd.merge(full, dashboard_df, left_on='id', right_on='id', how='left')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dashboard_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dashboard_df.to_csv('dashboard_df.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}